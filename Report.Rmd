---
title: An introduction into Statistical Theory Methods related to Systematic Equity
  Futures Statistical Arbitrage
author: "Jerry Xin"
date: "2023-12-12"
geometry: "left=1cm,right=1cm,top=1.5cm,bottom=1.5cm"
output: 
  pdf_document:
    fig_caption: yes
warnings: false
fontsize: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Background

To many, trading stocks, futures, and options in Financial Markets seems like a task that is completely based on luck, and is simply a test of one's intuition. Eugene Fama famously hypothesized in his paper â€œEfficient Capital Markets: a Review of Theory and Empirical Work" that markets are information efficient, which means that all future events and predictions are "priced in" to the current price. That is, it is almost impossible for one to trade favorably in the market, since all information is well known by other traders (Cochrane 2023). However, whether you believe the efficient market hypothesis or not, there are still multitudes of statistical methods used to analyze the market. Financial data is well known to be some of the most noisy, complex, and large in quantity data out there. Many independent trading firms have used these since the 1970s and even still today. These statistical and mathematical methods are really statistical in nature. That is, we care about approximating and efficiency in our methods. After all, a method that takes 1 week to process will never be useful since the stock price will already be long gone from what it was a week ago. Of course, one cannot predict major market catastrophes, such as the COVID-19 pandemic or the 2007 recession, and so methods most of the time try to minimize risk, but not reduce it to 0, and have a slight edge in 99% of market scenarios. 

To understand how stock prices generally move, we need to go over some background. Stocks generally trade during trading hours and after hours, and have different prices over different times, driven primarily by the supply and demand of the stock. Supply and demand can be driven by hundreds, even thousands of predictors, and is very hard to model. The data is of the nature of a time series data, as for each point in time, there is a price associated with it for a specific stock. Instead of analyzing the data through the "eye test" or intuition, we can introduce statistical concepts to help use analyze the data in a methodological and efficient manner.

The simplest statistical concept used in markets is the moving average. The moving average, over a period of 5 days to 10 days, allows us to look at the larger trends in the movements of a stock price without the noise or meaningless variance of the stock. A simple stock trader might understand how to use a moving average to look at a stock, but an experienced trader might look at multiple moving averages over different timeframes, periods of stocks, different statistics, and even other stocks highly correlated with the target stock.

### Research Objective

This study aims to analyze financial data for the 5 largest cap stocks for different industries, grouped by the grouping from the Dow Jones Industrial Average, as well as ETFs, and to discuss how to statistically determine if it is possible to trade multple equity futures using the Pairs Trading Algorithm and mean reversion. It will offer a comprehensive and customizable approach that analyzes different statistical concepts and statistical tests within a typical Pairs Trading Algorithm methodology. We will discuss the Johansen Test for Cointegration and the Augmented Dickey-Fuller Test for stationarity, and how to optimize a linear combination of stocks and standardize this, with the end goal of being able to develop a trading signal, both statistically and visually. Our goal is to predict whether it is possible to trade multiple stocks within our dataset using a Pairs Trading mean reversion strategy, and why it is or is not recommended.

### Dataset Overview
The data is taken from Yahoo and looks at stock data from December 12th, 2021 to December 12th, 2023, within a 2 year period. We look at the backward adjusted prices, since we need to account for inflation. When sampling data for a specific stock, one can specify the timeframe and the symbol of the stock. We can also extract information for ETFs, or exchange traded funds, which are combinations of stocks. Stock data is time series data, and has 1 observation for each trading day, which includes most weekdays but not holidays. The stock data includes the open price, high price, low price, close price, volume price, and adjusted price for the stock for each observation on a trading day. For each stock, because we are looking at data from December 12th 2021 to December 12th 2023, this will result in 502 observations per stock, where each observation is one trading day.

Keep in mind that we are not limited to the stocks that we select for the case study. In fact, we are encouraged to actively explore new combinations of ETFs, stocks, and other derivatives to find an optimal combination that you are okay with trading and holding as assets.

### Exploratory Data Analysis: Dow Jones Industrial Average Sectors
Our exploratory data analysis starts by looking at the Dow Jones Industrial Average and looking at the top five companies that make up each sector in the Dow Jones. From there we applied the adjusted prices of each of the top five stocks in each sector over time for our two-year period. Looking at the top five stocks for each sector, we wanted to find sectors that were very correlated in movement with the top five stocks. This is through the eye test by looking to see if the stock patterns over the last 2 years look somewhat similar within upward moves lower moves and in general variability. The full EDA is in Appendix E.

It's important that we take stocks that look to be very correlated with each other because we are trading these stocks against each other so if stocks within a certain industry are not correlated we will not do further analysis on these five stocks in a certain industry.It is also very important to note that using a foundational method such as the Dow Jones Industrial Average grouping of stocks into Industries is very important for pairs trading this is because we want stocks not just to be mathematically related but we also want some sort of casual or fundamental relationship between stocks are related. Thinking about statistically with the thousands of stocks that are out there they are bound to be too unrelated things such as a commodity and a random International stock that have the same price movement but we are very certain that these stocks will diverge in the future or they just happen to be by chance very very cointegrated in the time period that we are sampling for. 

The purpose of the EDA is twofold. First, we get a general understanding of industry sectors through the Dow Jones and certain ways on how to group stocks in specific baskets. Next, we also look at the top five stocks for each predefined industry sector and look for possible cointegration relationships between the top five stocks. We can therefore narrow down industries and corresponding stocks that we select for further analysis. 

We show a graph of our Eda below it relates the stocks prices of Microsoft, Apple, Amazon, Google(GOOGL), Google(GOOG), and Meta in a graph over 2022 and 2023. We can see relative similarity in the price movements for all these companies, and because they are all from big tech/ Information Technology, we know the price movement similarity isn't likely due to pure chance. We can also see that there is quite a bit of variance in the movement of prices, which if we are trying to perform arbitrage trading, is good because then there are more opportunities for "mismatched prices" that we can theoretically trade on for arbitrage. But more on this later in the methodology.

```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringr)
library(lubridate)
library(knitr)
library(yfR)
library(urca)
library(quantmod)
library(tseries)
```

```{r echo=FALSE, message=FALSE, warning= FALSE, results =FALSE, message=FALSE}
# Define stock symbols
stock_symbols <- c("MSFT", "AAPL", "AMZN", "GOOG", "GOOGL", "META")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))
```
```{r echo=FALSE, warning= FALSE}
# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices IT Companies",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```


## Methodology

### Selection of Baskets of Stocks
As explained in the Eda section we select baskets of stocks based on the predefined categories for Industries in the Dow Jones Industrial Average. From there we plotted the top five stocks by market cap in each industry sector, and looked at the graphs of price versus time to see if there was any correlation in movements of prices. This can be upswings, downswings and in general variation of price.

The IT companies in the Dow Jones are : "AAPL": Apple, "MSFT": Microsoft, "GOOGL": Google(voting rights), "META": Meta, "AMZN": Amazon, "GOOG": Google(No Voting right). The utilities companies in the Dow Jones include "NEE": NextEra Energy, "DUK": Duke Energy, "D": Dominion Energy, "SO": Southern Company, and "EXC": Exelon Corporation. The materials companies consist of "SHW": Sherwin-Williams, "LIN": Linde plc, "DOW": Dow Inc., "APD": Air Products and Chemicals, and "ECL": Ecolab Inc. The energy sector comprises "XOM": ExxonMobil, "CVX": Chevron Corporation, "COP": ConocoPhillips, "KMI": Kinder Morgan, and "SLB": Schlumberger Limited. Lastly, the ETFs we analyzed were SPY: A derivative of the S and P 500, VOO: The Vanguard 500 nETF index fund based on S and P 500, and IVV: the Ishares ETF based on the S and P 500.

### Stationarity
One of the most fundamental concepts we need to verify is stationarity. The concept of stationarity in statistics is used very frequently in Time series analysis. The general assumption of stationarity is that the parameters of our data generating process do not change over time.

Suppose our data is a stock that is increasing every year and we are not adjusting in the price based on inflation. Think of a line with slope 2. Because our data is not stationary, this means that certain statistics that would be meaningful for stationary data now cannot be consistently interpreted in a statistical manner, and assumptions are not met.

To test for stationarity, we want to use the augmented Dicky Fuller test. We want to test for something called a unit root and want to use an auto regressive unit root test. If the p-value is less than our cutoff value at Alpha level 0.05, then we can assume this series is likely stationary and if the alpha value is greater than 0.05, we can assume that the series is non-stationary.

### Cointegration

Something equally important we want to test for is co-integration. What we really want is to measure is some sort of similarity measure to compare time series of prices for stocks. As we saw in the EDA of the top 5 stocks in an industry against each other, we want something that is that has similar price movements, and we define similar here as cointegrated. This concept is very unfamiliar to many in the statistics field but a somewhat comparable concept is correlation. However, we argue here that we want to test for cointegration over correlation. Correlations between stocks and finance have been notoriously known to be unstable and are known to be bad measures of similarity in the long run.

The co-integration test procedure consists of using something called the Johansen test for cointegrating time series analysis. The basic idea of this test is that you test for a unit root in each component series individually unit using unit tests and if this unit cannot be rejected the next step is to test go integration among the components. Other versions of tests for co-integration can include Engle-Granger or Phillips-Ouliaris.

Correlation and co-integration, while they can be theoretically similar, are very different concepts. The graph in appendix F demonstrates this, as it is a graph of the cumulative sum of two normal random variables, one with a mean of 1 and the other with a mean of 2. If one were to calculate the correlation between these two time series, it would be around 0.99. However if one were to calculate the co-integration between these values p-value would be not significant at p = 0.26. This means that these 2 series are definitely correlated, but not co-integrated.

### Putting it all together

Putting it all together, our methodology for analyzing a basket of stocks for Systematic Equity Statistical Arbitrage using Pairs trading would be first to get the individual stocks within each basket using their symbols. Next, we isolate the adjusted price and we run a Johansen test to test for co-integration and obtain both cutoffs and eigenvectors. We use these eigenvectors to obtain the optimal linear combination of stocks by using the first eigenvector. Next, we run Augmented Dickey Fuller to test for stationarity and plot the combination of stocks to check if it looks stationary and cointegrated. Last, we standardize this using a z-score method by taking the combination minus the mean over the standard deviation. We graph lines in our plot, specifically add a red line at +2 standard deviations, and a green line -2 standard deviations. We also have a black line in the center that represents the mean, and we assume there will be mean reversion. This will symbolize to sell when the stock is above the red line and buy on the stock is below the red line, since these will be considered major deviations from the mean, rather than noise, and we expect the combination of stocks to eventually revert to the mean value or black line. 



```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
today <- as.Date("2023-12-12")
months_prior <- today - (365*2)
df <- read_csv("data/sp500.csv")
```


```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
# Information Technology
it_companies <- c("AAPL", "MSFT", "GOOGL", "META", "AMZN", "GOOG")

# Health Care
healthcare_companies <- c("JNJ", "PFE", "MRK", "ABBV", "LLY")

# Financials
financial_companies <- c("JPM", "BAC", "WFC", "C", "GS")

# Consumer Discretionary
discretionary_companies <- c("AMZN", "DIS", "NKE", "HD", "MCD")

# Communication Services
communication_companies <- c("FB", "GOOGL", "GOOG", "T", "VZ")

# Industrials
industrial_companies <- c("BA", "HON", "UNP", "MMM", "CAT")

# Consumer Staples
staples_companies <- c("PG", "KO", "WMT", "PEP", "COST")

# Utilities
utilities_companies <- c("NEE", "DUK", "D", "SO", "EXC")

# Materials 
materials_companies <- c("SHW", "LIN", "DOW", "APD", "ECL")

# Real Estate
real_estate_companies <- c("AMT", "SPG", "PLD", "EQIX", "CCI")

# Energy 
energy_companies <- c("XOM", "CVX", "COP", "KMI", "SLB")

getSymbols(it_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols("SPY", from="2021-12-12", to="2023-12-12")
getSymbols("IVV", from="2021-12-12", to="2023-12-12")
getSymbols("VOO", from="2021-12-12", to="2023-12-12")

getSymbols(materials_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols(energy_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols(utilities_companies, from = "2021-12-12", to = "2023-12-12")
```

## Results 

### IT Companies

Our results from the IT companies, the basket of stocks of which we are focusing on in this case study, are below. Further results from the other baskets of stocks- the ETFs, material companies, energy companies, and utility companies will be provided in the appendix but interpreted at the end of the results section.

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(it_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- it_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")
```
First, we display the Eigenvalues in the first table, which correspond to each stock in our basket of stocks. In this case, we are looking at "AAPL": Apple, "MSFT": Microsoft, "GOOGL": Google(Voting rights), "META": Meta, "AMZN": Amazon, "GOOG": Google(No voting rights)

Next, we address the issue of cointegration by displaying the results from the Johansen test in the second table below, with the second table showing the test statistic for the null hypotheses of r<=5, r<=4, r<=3, r<=2, r<=1, and r = 0. An exmaple of how to interpret one of these null hypotheses, for example r<=2, is that if we have sufficient evidence to suggest that we should reject the null, we have evidence to suggest that the rank of the eigenvalue matrix is 3, so NOT rank less than or equal to 3, and therefore we need a linear combination of 3 time series to form a stationary series. Additionally, we have the cutoff values of the test statistics for significance levels of alpha = 0.01, 0.05, and 0.10.

At a significance level of a= 0.05, we can see that the test of R <= 1 is statistically significant, so we can reject the null hypothesis and this suggests that the rank is 2 or greater, on average

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]
```

Next, we display the third table of eignevectors, normalized to the first column. There are also known as the cointegration relations. To find the linear combination of time series to create a stationary series, we need to take the normalized eigenvector column corresponding to the stock with the largest eigenvalue, and use that as our linear combination for our stocks. For example, if that column was [1,3,-2], our linear combination would be 1 * stock A + 3 * stock B - 2 * stock C.

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

Next, after we obtain the optimized linear combination of our six stocks, we combine these into a combined series and graph this combination as a function of time. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

Lastly, we standardize our combined series by simply using a z-score standardization method. We subtract the mean and divide by the standard deviation to obtain these scores. We then plot this standardized combined series as a Time series and add a red line for two standard deviations above the mean of 0 and green line for standard deviations below the mean of zero. We could also add a red or green line for 1 standard deviation above/below, as the number of standard deviation is not something that is necessarily fixed. The trading idea is that if the series is below the green line we would expect to buy in this case because we expected to revert back to approximately zero, the black line, and if the series is above the two line we would expect to sell in this case because we would also expected to revert to approximately, zero and we know that the movement above zero and below zero is not a not simply noise, because we are 2 standard deviations above it or to standard deviations below it. This indicates a market mispricing, and we can take advantage of this. Of course, we must use historical data to backtest if our linear combination is in fact stationary, and always aknowledge that past performance is never trul indicative of future performance, and it would be unreasonable to expect stationarity to hold in once-in-a-lifetime events that are unprecedented, such as a global pandemic like COVID-19.

### ETF

Interpreting our results for the ETF basket of 3 stocks is similar. The results are found in Appendix A.At a significance level of a= 0.05, we can see that the test of R <= 1 is statistically significant, so we can reject the null hypothesis and this suggests that the rank is 2 or greater, on average. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

### Materials

Interpreting our results for the materials basket of 5 stocks is similar. The results are found in Appendix B. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is .0449943, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

The materials companies is a unique case where just because the companies pass the "eye-test" for cointegration/ general similar price movement patterns, we can see that the optimized combined series is by all means most likely not stationary. That is compeltely ok however, since as stock traders we are ok with not trading on this opportunity and looking for others that are most likely to be stationary and exhibit more mean reversion, and make us more money on average with lower risk/downside.

### Energy

Interpreting our results for the energy basket of 5 stocks is similar. The results are found in Appendix C. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

### Utilities

Interpreting our results for the utilities basket of 5 stocks is similar. The results are found in Appendix D. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is .0739892, which at a significant level of a = 0.05, we cannot reject the hypothesis of a unit root and therefore this suggests that we do not have a stationary series formed by our linear combination.

## Discussion

### Research Objective

Our research objective was to broadly look at different combinations of different stocks for each industry, grouped by some relation and to see if it is statistically possible to determine or optimize how to trade a combination or a linear combination of these stocks, systematically using a pairs trading algorithm and mean reversion. Lastly we wanted to determine, if a series or linear combination is co-integrated and stationary, how and when to develop trading signals to buy or sell our optimized combination of stocks.

We accomplished our research objective by looking at the various baskets of stocks grouped by the Dow Jones Industrial Average. Specifically, we analyzed the top five largest stocks in each category. In our exploratory data analysis, we graph the top five largest stocks for each industry as a function of price versus time to see if there are any noticeable patterns, using the eye test for the categories that we notice patterns in. The categories ended up being IT companies, energy companies, materials companies, utilities companies, and lastly ETFs. The specific stocks for these categories are discussed in the methodology section. Next, to test for stationarity and co-integration, we specifically use the Johansen test for co-integration and the augmented Dickey-Fuller test for stationarity. We are able to specifically run the Johansen test for co-integration in the augmented Dicky Fuller test for stationary for each of our baskets of the largest five stocks in each industry that we select. 

As a result of the Johansen test for co-integration we obtain a matrix of different null hypotheses for the rank of our Matrix. This indicates how many stocks are needed in a linear combination to form a stationary series, and we also have the test statistic for each of these null hypotheses and the cutoff value for different levels of alpha- specifically 0.05, 0.01, and 0.10. Next we also obtain a matrix of eigenvectors normalized to the first column. The first column represents the optimal linear combination of stocks for a co-integrated series. Next, we run an augmented Dickey-Fuller test to test if our series is in fact stationary. Lastly, we standardize our combined stock using a z-score transformation, and plot lines at 2 and -2 standard deviations, to indicate to buy when below -2 standard deviations and to sell when above the line at 2 standard deviations.

We conclude that the IT companies and the ETFs are cointegrated and stationary, while the materials and energy companies are not cointegrated but stationary, and the utilities companies are cointegrated but not stationary. Overall, the successful results passed the eye test, while the unsuccessful results also seem to pass the eye test from the EDA, but after further analysis just did not meet the threshold for co-integration or stationarity.

### Limitations of Data and Methodology

A key limitation was that we did not look at every possible combination of stock, which can be 2^n, where n is the number of stocks possibilities. Even a computer with all of the instructions to run 2^n combinations will not be able to evaluate every possible combination. Instead, we try to use a lot of intuition, as well as EDA to determine ideal baskets of stocks to analyze. 

It is important to note on the flip side however, that are method of testing every possible combination of stock is also not satisfactory. In fact if we were to look at two to the power of n combinations, we'd expect some stocks combinations to be perfectly aligned where the stocks have almost no relation to one another. It is actually important in this case to use our intuition and our background knowledge to understand why two stocks are behaving similarly, because in the case that the two stocks are actually simply by chance moving similarly in our time frame, but not actually related to each other, we would be at risk for a huge losses.It is also important for us to analyze many different time frames, and in this case we only looked at 2 years of data. If possible, we should separate stock data into training and testing time windows to test if our strategies hold over different periods of different economic conditions. It's incredibly important to also note that past performance is never indicative of future performance for stocks and this should always be under consideration when using a algorithm based on past data to trade on potential predicted future data. 

Lastly, we could look at different stock prices as predictors, such as the open, the max price, min price, and even stock volume. None of these factors were considered. To sum it up, we need to be incredibly rigorous in our analyses, both quantitatively and qualitatively. We need to make sure that our combination of stocks makes sense and are related to each other somehow through intuition, quantitatively verify stationarity and co-integration, possibly using multiple tests. It's important to know that we're using an alpha level of 0.05 and this is very arbitrarily set. In a field where we're trading stocks with real money and realizing our profits and losses, we expect a lot more variance and for past performance not necessarily be indicative of future performance. Therefore it is possible to adjust our Alpha level to possibly 0.01 or less, to minimize the risk of error. We need to then tests our findings on independent datasets, ideally split through a train test split, as well as different time frames and possibly with different price variables. Lastly, we should consider a unique economic situation somehow in our model as well.

Some alternatives for tests to the Johansen test for co-integration include the Engle-Granger or Phillips-Ouliaris tests. Some alternatives for the backward adjusted prices could include a adjusted open, max, min, or close. We could also look at volume as a separte predictor. Catastrophic events we need to consider include a recession such as 2008, a pandemic such as the COVID-19 pandemic, or even something such as the Russia-Ukraine War. We can also use a train-test split method to analyze and cross-validate different time frames, such as 5 years, 10 years, or even 20 years of stock data.

### The future of Stock Trading:Automation & Arbitrage

Most future work can be done in finding a balance between automating a computer to find different baskets of stocks to test, as well as integrating human intuition about stocks into this search method. After all, it makes more sense for computer to have a general outline of companies from a human, and evaluate from there. We also want more rigor in our tests for stationarity and co-integration. Currently, if a stock is co integrated and stationary, we are willing to trade on it. We might want to look at a more factors. We want to automate data extraction from a reliable source such as Bloomberg, as well as test different time periods and cross validate.

Ideally we'd also want to add a method to be able to visualize our data better, both within EDA and after running our test for stationarity and co-integration. Additionally, we would want to create a signal from our graph to be able to send to a trading bot, ideally with the lowest latency possible, to execute trades in real time. We would also want to be able to input or price data in real time as well, so that if the price data exceeds are z-score threshold we are able to sell in real time, and if our price data is below are the whole threshold we are able to buy in real time. In line with creating a systematic trading bot, we also would want to add some sort of human checkpoint to our systematic Trading system.

We would want some sort of disaster or worst case scenario analysis for incredibly impossible Market movements such as the covid-19 pandemic or recession as well. Lastly, as many other firms are starting to do systematic trading as well, it would also be helpful to have a general scouting report on what other firms are trading. This is so that we are not eating each other's profits up and directly competing with another person or company trading the same stocks. Some ideas also used in trading are using Kernal Density Estimation(KDEs) to be able to model stocks and perform pairs trading statistical arbitrage.

### Final Thoughts

This study is but the tip of the iceberg for a systematic Statistical Arbitrage pairs trading strategy guide. With everything that's discussed in the limitations, it seems that their infinite possibilities to improve our strategy, and our current process is nothing but a toy dataset or model that will not hold up in the modern, fast moving markets. However, it is important to note that we have discussed something very important, which is the general foundation of which this these strategies are created on. This is extremely important going forward, in not only in creating your own personalized strategy for arbitrage, which may or may not be successful and probably will be less successful over time, but also in your understanding of statistics and Time series beyond the scope of trading or financial markets. The concepts of EDA, using your intuition to pick stocks and predictor variables, as well as cointegration and stationarity, hold true beyond the idea of Statistical Arbitrage Trading and these will surely be expanded upon no matter what career you pursue.

## Appendix A: ETFs

```{r , echo=FALSE, warning= FALSE, message=FALSE}
spyAdj = unclass(SPY$SPY.Adjusted)
ivvAdj = unclass(IVV$IVV.Adjusted)
vooAdj = unclass(VOO$VOO.Adjusted)

jotest=ca.jo(data.frame(spyAdj,ivvAdj,vooAdj), type="trace", K=2, ecdet="none", spec="longrun")
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- c("SPY", "IVV", "VOO")
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat,jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10","a = 0.05","a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

combo = jotest@V[,1][1]*spyAdj + jotest@V[,1][2]*ivvAdj + jotest@V[,1][3]*vooAdj
plot(combo, type="l")

adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type="l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of stock S over time")

abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2,2,1), lwd = c(2, 2, 2))
```

## Appendix B: Materials 

```{r , echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(materials_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- materials_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```
## Appendix C: Energy Companies

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(energy_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- energy_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

## Appendix D: Utility Companies

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(utilities_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- utilities_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

## Appendix E: EDA

```{r}
# Define stock symbols
stock_symbols <- c("MSFT", "AAPL", "AMZN", "GOOG", "GOOGL", "META")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices IT Companies",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("JNJ", "PFE", "MRK", "ABBV", "LLY")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Health Care",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("SPY", "IVV", "VOO")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices ETFs",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("JPM", "BAC", "WFC", "C", "GS")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Financials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```


```{r}
stock_symbols <- c("AMZN", "DIS", "NKE", "HD", "MCD")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Consumer Discretionary",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("META", "GOOGL", "GOOG", "T", "VZ")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Communication Services",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("BA", "HON", "UNP", "MMM", "CAT")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Industrials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("PG", "KO", "WMT", "PEP", "COST")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Consumer Staples",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("SHW", "LIN", "DOW", "APD", "ECL")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Utilities",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("NEE", "DUK", "D", "SO", "EXC")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Materials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("AMT", "SPG", "PLD", "EQIX", "CCI")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Real Estate",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("XOM", "CVX", "COP", "KMI", "SLB")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Energy",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

## Appendix F: Graph Example of Cointegration vs Correlation
```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
set.seed(123)  # Setting seed for reproducibility
X_returns <- rnorm(100, mean = 1, sd = 1)
Y_returns <- rnorm(100, mean = 2, sd = 1)

X_diverging <- cumsum(X_returns)
Y_diverging <- cumsum(Y_returns)

# Create a data frame
diverging_data <- data.frame(X = X_diverging, Y = Y_diverging)

ggplot(diverging_data, aes(x = seq_along(X), y = X, color = "Mean 1 Cumulative returns")) +
  geom_line() +
  geom_line(aes(y = Y, color = "Mean 2 Cumulative returns")) +
  xlim(0, 99) +
  labs(title = "Diverging Time Series", x = "Time", y = "Cumulative Returns") +
  theme_minimal()
```

\newpage
### Works Cited
www.chicagobooth.edu/review/eugene-fama-efficient-markets-and-the-nobel-prize#:~:text=In%201970%2C%20in%20%E2%80%9CEfficient%20Capital,available%20information%20about%20future%20values

https://chat.openai.com/

Prompts: How to interpret a Johansen Test in R, how to intepret and run a Dickey-Fuller Test, how to use Kable in R, how to extract stock data.

https://stats.stackexchange.com/questions/268122/cointegration-coefficients-with-r-ca-jo-function

https://www.quantstart.com/articles/Johansen-Test-for-Cointegrating-Time-Series-Analysis-in-R/

https://github.com/KidQuant/Pairs-Trading-With-Python/blob/master/PairsTrading.ipynb