---
title: An introduction into Statistical Theory Methods related to Systematic Equity
  Futures Statistical Arbitrage
author: "Jerry Xin"
date: "2023-12-12"
geometry: "left=2cm,right=2cm,top=1.5cm,bottom=1.5cm"
output: 
  pdf_document:
    fig_caption: yes
warnings: false
fontsize: 10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Background

To many, trading stocks, futures, and options in Financial Markets seems like a task that is completely based on luck, and is simply a test of one's intuition. Eugene Fama famously hypothesized in his paper “Efficient Capital Markets: a Review of Theory and Empirical Work" that markets are information efficient, which means that all future events and predictions are "priced in" to the current price. That is, it is almost impossible for one to trade favorably in the market, since all information is well known by other traders (Cochrane 2023). However, whether you believe the efficient market hypothesis or not, there are still multitudes of statistical methods used to analyze the market. Financial data is well known to be some of the most noisy, complex, and large in quantity data out there. Many independent trading firms have used these since the 1970s and even still today. These statistical and mathematical methods are really statistical in nature. That is, we care about approximating and efficiency in our methods. After all, a method that takes 1 week to process will never be useful since the stock price will already be long gone from what it was a week ago. Of course, one cannot predict major market catastrophes, such as the COVID-19 pandemic or the 2007 recession, and so methods most of the time try to minimize risk, but not reduce it to 0, and have a slight edge in 99% of market scenarios. 

To understand how stock prices generally move, we need to go over some background. Stocks generally trade during trading hours and after hours, and have different prices over different times, driven primarily by the supply and demand of the stock. Supply and demand can be driven by hundreds, even thousands of predictors, and is very hard to model. The data is of the nature of a time series data, as for each point in time, there is a price associated with it for a specific stock. Instead of analyzing the data through the "eye test" or intuition, we can introduce statistical concepts to help use analyze the data in a methodological and efficient manner.

The simplest statistical concept used in markets is the moving average. The moving average, over a period of 5 days to 10 days, allows us to look at the larger trends in the movements of a stock price without the noise or meaningless variance of the stock. A simple stock trader might understand how to use a moving average to look at a stock, but an experienced trader might look at multiple moving averages over different timeframes, periods of stocks, different statistics, and even other stocks highly correlated with the target stock.

### Research Objective

This study aims to analyze financial data for large cap stocks and to discuss how to statistically determine if it is possible to trade 2 equity futures using the Pairs Trading Algorithm and mean reversion. It will offer a comprehensive and customizable approach that analyzes different statistical concepts and statistical tests within a typical Pairs Trading Algorithm methodology. Our goal is to predict whether it is possible to trade 2 stocks within our dataset using a Pairs Trading mean reversion strategy, and why it is or is not recommended.

### Dataset Overview
The data is taken from Yahoo and looks at stock data from December 12th, 2021 to December 12th, 2023, within a 2 year period. We look at the backward adjusted prices, since we need to account for inflation. When sampling data for a specific stock, one can specify the timeframe and the symbol of the stock. We can also extract information for ETFs, or exchange traded funds, which are combinations of stocks. Stock data is time series data, and has 1 observation for each trading day, which includes most weekdays but not holidays. The stock data includes the open price, high price, low price, close price, volume price, and adjusted price for the stock for each observation on a trading day. For each stock, because we are looking at data from December 12th 2021 to December 12th 2023, this will result in 502 observations per stock, where each observation is one trading day.

Keep in mind that we are not limited to the stocks that we select for the case study. In fact, we are encouraged to actively explore new combinations of ETFs, stocks, and other derivatives to find an optimal combination that you are okay with trading and holding as assets.

### Exploratory Data Analysis: Dow Jones Industrial Average Sectors
Our exploratory data analysis starts by looking at the Dow Jones Industrial Average and looking at the top five companies that make up each sector in the Dow Jones. From there we applied the adjusted prices of each of the top five stocks in each sector over time for our two-year period. Looking at the top five stocks for each sector, we wanted to find sectors that were very correlated in movement with the top five stocks. This is through the eye test by looking to see if the stock patterns over the last 2 years look somewhat similar within upward moves lower moves and in general variability. The full EDA is in Appendix E.

It's important that we take stocks that look to be very correlated with each other because we are trading these stocks against each other so if stocks within a certain industry are not correlated we will not do further analysis on these five stocks in a certain industry.It is also very important to note that using a foundational method such as the Dow Jones Industrial Average grouping of stocks into Industries is very important for pairs trading this is because we want stocks not just to be mathematically related but we also want some sort of casual or fundamental relationship between stocks are related. Thinking about statistically with the thousands of stocks that are out there they are bound to be too unrelated things such as a commodity and a random International stock that have the same price movement but we are very certain that these stocks will diverge in the future or they just happen to be by chance very very cointegrated in the time period that we are sampling for. 

The purpose of the EDA is twofold. First, we get a general understanding of industry sectors through the Dow Jones and certain ways on how to group stocks in specific baskets. Next, we also look at the top five stocks for each predefined industry sector and look for possible cointegration relationships between the top five stocks. We can therefore narrow down industries and corresponding stocks that we select for further analysis. 

We show a graph of our Eda below it relates the stocks prices of Microsoft, Apple, Amazon, Google(GOOGL), Google(GOOG), and Meta in a graph over 2022 and 2023. We can see relative similarity in the price movements for all these companies, and because they are all from big tech/ Information Technology, we know the price movement similarity isn't likely due to pure chance. We can also see that there is quite a bit of variance in the movement of prices, which if we are trying to perform arbitrage trading, is good because then there are more opportunities for "mismatched prices" that we can theoretically trade on for arbitrage. But more on this later in the methodology.

```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringr)
library(lubridate)
library(knitr)
library(yfR)
library(urca)
library(quantmod)
library(tseries)
```

```{r echo=FALSE, message=FALSE, warning= FALSE, results =FALSE, message=FALSE}
# Define stock symbols
stock_symbols <- c("MSFT", "AAPL", "AMZN", "GOOG", "GOOGL", "META")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))
```
```{r echo=FALSE, warning= FALSE}
# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices IT Companies",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```


## Methodology

### Selection of Baskets of Stocks
As explained in the Eda section we select baskets of stocks based on the predefined categories for Industries in the Dow Jones Industrial Average. From there we plotted the top five stocks by market cap in each industry sector, and looked at the graphs of price versus time to see if there was any correlation in movements of prices. This can be upswings, downswings and in general variation of price.

### Stationarity
One of the most fundamental concepts we need to verify is stationarity. The concept of stationarity in statistics is used very frequently in Time series analysis. The general assumption of stationarity is that the parameters of our data generating process do not change over time.

Suppose our data is a stock that is increasing every year and we are not adjusting in the price based on inflation. Think of a line with slope 2. Because our data is not stationary, this means that certain statistics that would be meaningful for stationary data now cannot be consistently interpreted in a statistical manner, and assumptions are not met.

To test for stationarity, we want to use the augmented Dicky Fuller test. We want to test for something called a unit root and want to use an auto regressive unit root test. If the p-value is less than our cutoff value at Alpha level 0.05, then we can assume this series is likely stationary and if the alpha value is greater than 0.05, we can assume that the series is non-stationary.

### Cointegration

Something equally important we want to test for is co-integration. What we really want is to measure is some sort of similarity measure to compare time series of prices for stocks. As we saw in the EDA of the top 5 stocks in an industry against each other, we want something that is that has similar price movements, and we define similar here as cointegrated. This concept is very unfamiliar to many in the statistics field but a somewhat comparable concept is correlation. However, we argue here that we want to test for cointegration over correlation. Correlations between stocks and finance have been notoriously known to be unstable and are known to be bad measures of similarity in the long run.

The co-integration test procedure consists of using something called the Johansen test for cointegrating time series analysis. The basic idea of this test is that you test for a unit root in each component series individually unit using unit tests and if this unit cannot be rejected the next step is to test go integration among the components. Other versions of tests for co-integration can include Engle-Granger or Phillips-Ouliaris.

Correlation and co-integration, while they can be theoretically similar, are very different concepts. The graph in appendix F demonstrates this, as it is a graph of the cumulative sum of two normal random variables, one with a mean of 1 and the other with a mean of 2. If one were to calculate the correlation between these two time series, it would be around 0.99. However if one were to calculate the co-integration between these values p-value would be not significant at p = 0.26. This means that these 2 series are definitely correlated, but not co-integrated.

### Putting it all together

Putting it all together, our methodology for analyzing a basket of stocks for Systematic Equity Statistical Arbitrage using Pairs trading would be first to get the individual stocks within each basket using their symbols. Next, we isolate the adjusted price and we run a Johansen test to test for co-integration and obtain both cutoffs and eigenvectors. We use these eigenvectors to obtain the optimal linear combination of stocks by using the first eigenvector. Next, we run Augmented Dickey Fuller to test for stationarity and plot the combination of stocks to check if it looks stationary and cointegrated. Last, we standardize this using a z-score method by taking the combination minus the mean over the standard deviation. We graph lines in our plot, specifically add a red line at +2 standard deviations, and a green line -2 standard deviations. We also have a black line in the center that represents the mean, and we assume there will be mean reversion. This will symbolize to sell when the stock is above the red line and buy on the stock is below the red line, since these will be considered major deviations from the mean, rather than noise, and we expect the combination of stocks to eventually revert to the mean value or black line. 



```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
today <- as.Date("2023-12-12")
months_prior <- today - (365*2)
df <- read_csv("data/sp500.csv")
```


```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
# Information Technology
it_companies <- c("AAPL", "MSFT", "GOOGL", "META", "AMZN", "GOOG")

# Health Care
healthcare_companies <- c("JNJ", "PFE", "MRK", "ABBV", "LLY")

# Financials
financial_companies <- c("JPM", "BAC", "WFC", "C", "GS")

# Consumer Discretionary
discretionary_companies <- c("AMZN", "DIS", "NKE", "HD", "MCD")

# Communication Services
communication_companies <- c("FB", "GOOGL", "GOOG", "T", "VZ")

# Industrials
industrial_companies <- c("BA", "HON", "UNP", "MMM", "CAT")

# Consumer Staples
staples_companies <- c("PG", "KO", "WMT", "PEP", "COST")

# Utilities
utilities_companies <- c("NEE", "DUK", "D", "SO", "EXC")

# Materials 
materials_companies <- c("SHW", "LIN", "DOW", "APD", "ECL")

# Real Estate
real_estate_companies <- c("AMT", "SPG", "PLD", "EQIX", "CCI")

# Energy 
energy_companies <- c("XOM", "CVX", "COP", "KMI", "SLB")

getSymbols(it_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols("SPY", from="2021-12-12", to="2023-12-12")
getSymbols("IVV", from="2021-12-12", to="2023-12-12")
getSymbols("VOO", from="2021-12-12", to="2023-12-12")

getSymbols(materials_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols(energy_companies, from = "2021-12-12", to = "2023-12-12")
getSymbols(utilities_companies, from = "2021-12-12", to = "2023-12-12")
```

## Results 

### IT Companies

Our results from the IT companies, the basket of stocks of which we are focusing on in this case study, are below. Further results from the other baskets of stocks- the ETFs, material companies, energy companies, and utility companies will be provided in the appendix but interpreted at the end of the results section.

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(it_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- it_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")
```
First, we display the Eigenvalues in the first table, which correspond to each stock in our basket of stocks. In this case, we are looking at "AAPL": Apple, "MSFT": Microsoft, "GOOGL": Google(Voting rights), "META": Meta, "AMZN": Amazon, "GOOG": Google(No voting rights)

Next, we address the issue of cointegration by displaying the results from the Johansen test in the second table below, with the second table showing the test statistic for the null hypotheses of r<=5, r<=4, r<=3, r<=2, r<=1, and r = 0. An exmaple of how to interpret one of these null hypotheses, for example r<=2, is that if we have sufficient evidence to suggest that we should reject the null, we have evidence to suggest that the rank of the eigenvalue matrix is 3, so NOT rank less than or equal to 3, and therefore we need a linear combination of 3 time series to form a stationary series. Additionally, we have the cutoff values of the test statistics for significance levels of alpha = 0.01, 0.05, and 0.10.

At a significance level of a= 0.05, we can see that the test of R <= 1 is statistically significant, so we can reject the null hypothesis and this suggests that the rank is 2 or greater, on average

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]
```

Next, we display the third table of eignevectors, normalized to the first column. There are also known as the cointegration relations. To find the linear combination of time series to create a stationary series, we need to take the normalized eigenvector column corresponding to the stock with the largest eigenvalue, and use that as our linear combination for our stocks. For example, if that column was [1,3,-2], our linear combination would be 1 * stock A + 3 * stock B - 2 * stock C.

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

Next, after we obtain the optimized linear combination of our six stocks, we combine these into a combined series and graph this combination as a function of time. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

Lastly, we standardize our combined series by simply using a z-score standardization method. We subtract the mean and divide by the standard deviation to obtain these scores. We then plot this standardized combined series as a Time series and add a red line for two standard deviations above the mean of 0 and green line for standard deviations below the mean of zero. We could also add a red or green line for 1 standard deviation above/below, as the number of standard deviation is not something that is necessarily fixed. The trading idea is that if the series is below the green line we would expect to buy in this case because we expected to revert back to approximately zero, the black line, and if the series is above the two line we would expect to sell in this case because we would also expected to revert to approximately, zero and we know that the movement above zero and below zero is not a not simply noise, because we are 2 standard deviations above it or to standard deviations below it. This indicates a market mispricing, and we can take advantage of this. Of course, we must use historical data to backtest if our linear combination is in fact stationary, and always aknowledge that past performance is never trul indicative of future performance, and it would be unreasonable to expect stationarity to hold in once-in-a-lifetime events that are unprecedented, such as a global pandemic like COVID-19.

### ETF

Interpreting our results for the ETF basket of 3 stocks is similar. The results are found in Appendix A.At a significance level of a= 0.05, we can see that the test of R <= 1 is statistically significant, so we can reject the null hypothesis and this suggests that the rank is 2 or greater, on average. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

### Materials

Interpreting our results for the materials basket of 5 stocks is similar. The results are found in Appendix B. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is .0449943, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

The materials companies is a unique case where just because the companies pass the "eye-test" for cointegration/ general similar price movement patterns, we can see that the optimized combined series is by all means most likely not stationary. That is compeltely ok however, since as stock traders we are ok with not trading on this opportunity and looking for others that are most likely to be stationary and exhibit more mean reversion, and make us more money on average with lower risk/downside.

### Energy

Interpreting our results for the energy basket of 5 stocks is similar. The results are found in Appendix C. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is 0.01, which at a significant level of a = 0.05, we can reject the hypothesis of a unit root and suggest that we have a stationary series formed by our linear combination.

### Utilities

Interpreting our results for the utilities basket of 5 stocks is similar. The results are found in Appendix D. At a significance level of a= 0.05, we can see that none of the rank tests is statistically significant, so we do not have sufficient statistical evidence to reject any null hypotheses and this suggests that the rank is 0 or greater, on average. We should note that this gives us no extra useful information. After that we run the augmented Dickey-Fuller test to test for stationarity of our series. Our p-value for the Augmented Dickey-Fuller test is .0739892, which at a significant level of a = 0.05, we cannot reject the hypothesis of a unit root and therefore this suggests that we do not have a stationary series formed by our linear combination.

## Discussion

### Goals, Eye-test, Outside Research
Our goal was to recommend a team of 5 women and 5 men for the Olympic Games in Paris 2024, with an expected medal count for each team. We accomplished our goal of selecting athletes by first selecting 2 all-around athletes, and then finding 3 specialists to complement them, while adjusting for 'clutch' factor and recency by duplicating observations based on final vs qualifying rounds, and recency of competition. We then account for individual variance by creating a T-distribution for each competitor for each apparatus, and sampling from that distribution 1000 times. We also then find the predicted medal thresholds using repeated sampling from a T-distribution, taking the top 24 individual scorers, and compare our sampled individual results with the predicted thresholds to find predicted individual medal counts.

The ‘eye’ test confirms some of the choices made by our model. The choices of Simone Biles, Skye Blakely, Shilese Jones align with the choices of the US team in the most recent World Championships where the US women won a gold medal. Both Jones and Biles also medaled in the individual all-around event justifying them being chosen as the two guaranteed all-around competitors. Furthermore, an independent level 10 gymnast committed to a D1 university chose a team identical to that of the US women’s world championship squad leading us to believe the eye test gives ground for our choices. On the men’s side the model’s choices do align with the US men’s team that won bronze in the most recent World Championship. Paul Juda, Fred Richard, and Khoi Young are both on our suggested Olympic team and the 2023 World Championship team. Hong and Richard were the selected all-around competitors also justifying our model’s choices. 

Still, there is important information that the USA Olympic committee as well as some of the general public have that our model does not. For example, Konnor McClain may look like a strong choice, but she left Elite (which is a program designed to be a pathway to the USA national team) in 2023 in favor of college. Although she plans to continue training for the Olympics, her non-Elite status likely keeps her from being in the USA olympic committee’s player pool. A similar situation exists for Sunisa Lee, the 2020 Olympic all-around champion. Although her weighted scores are quite high and the model would select her as one of the all-around locks, she has not competed Elite since her enrollment in college in 2022 and a recent injury in which she gained over 50 pounds likely keeps her from making the USA national team. Injury concerns will also keep Brody Malone off the team on the men’s side. Although he eyes a spring 2024 comeback from a knee injury suffered in the spring of 2023, he will likely not be ready or in form for the Olympics. 

### A Qualitative Analysis of Team USA

A qualitative analysis of Team USA’s women’s gymnastics team has determined that Team USA will most likely pick from a pool of 7 women to choose their team. Only 7 women placed in the top three at any event in the most recent USA national gymnastics championships. These 7 women are Simone Biles, Shilese Jones, Leanne Wong, Skye Blakely, Kaliya Lincoln, Joscelyn Roberson, and Jade Carey. The two all-around staples are likely to be Simone Biles and Shilese Jones as both placed first and second in the USA championships as well as were the top 2 USA finishers in the World Championships. Leanne Wong has a case for being an all-around competitor, but in both championships she finished behind Biles and Jones. 

As for the specialists there is support for Leanne Wong being a third all-around athlete as that was the way the USA utilized her in the World Championships. However, she did not qualify for any individual final so her utilization as such an athlete in the Olympics may not be the most optimal strategy. If we choose three specialists, Skye Blakely is a lock to be a specialist. Not only did she place second in the vault, uneven bars, and balance beam, but she is far and away the USA’s third best uneven bars athlete along with Biles and Jones. Beyond these three athletes, the biggest hole lies in the Vault and Floor Exercise events. Of the three athletes remaining only Kaliya Lincoln fills the gap in the floor exercise as she not only silver medaled at the national championships but also won gold at the Pan-American games. Kayla DiCello is another option at Vault for the USA. While not as strong as Lincoln, if the USA is looking for another all-around gymnast for added depth, Dicello provides just that, not to mention her second place finish in floor at the Pan-American games. Still, Kaliya Lincoln’s floor abilities would make her a good fourth addition. That leaves Joscelyn Roberson and Jade Carey who are the two best options for the Vault. Given her recent form, Joscelyn Roberson has a slight edge over Jade Carey. Roberson won gold at the National Championships and had a good chance to medal at the World Championships before tweaking her left leg before the finals (which also prevented her from competing in the team all-around final). 

All in all, a qualitative analysis would suggest a five-woman team of Simone Biles, Shilese Jones, Skye Blakely, Kaliya Lincoln, and Joscelyn Roberson. Alternative lineups would feature either Kayla Dicello over Kaliya Lincoln or Jade Carey over Joscelyn Roberson.

A qualitative analysis for the Men’s Olympic team is much less clear. Frederick Richard would be a lock for the team. His bronze medal at the World Championships in the individual all-around while also topping the USA in all-around score in qualifying cements his spot as our top all-around athlete. The second spot would go to either Asher Hong or Khoi Young. Because Asher Hong won the National Championship and was chosen above Young in the World Championships the edge goes to him. 

Hong and Richard are both skillful at 4 of the 6 events, but neither covers the base of horizontal bar, so selecting an athlete for that event is important. The best option for this event is Paul Juda who made it to the horizontal bar finals round at the World Championships. Juda has also shown high levels of success in the floor exercise (won the National Championship) and the vault (made the World Championship final) making him an ideal third choice for the Olympic team. Khoi Young is the next best option to make the team. Not only did he place second in the individual all-around at the National Championships exemplifying his strength at all events, but he also got a silver medal in both the vault and pommel horse at the World Championships. Given the results of the National Championships two men are in consideration for the final spot: Donnell Whittenberg and Yul Moldauer. Both placed first in an event at the National Championships and third in another event. The slight edge, however, goes to Whittenberg who placed first in the rings, the only event in which the USA did not send a single athlete to the finals of the World championships, at the National Championships. While Moldauer has Olympic experience, Wittenberg’s expertise in the rings gives him a slight edge. 

That gives a final five-man team of Frederick Richard, Asher Hong, Paul Juda, Khoi Young, and Donnell Whittenberg.


### Limitations
There were also several limitations in our statistical methods. Firstly, we arbitrarily assigned weights to each of the decided ‘cutoffs’ it is more than possible that our weightings could result in the selection of a different team than if no weights were used. However, the weights used were for good reason given how recent performance affects future performance and our desire to pick more ‘clutch’ individuals. Another potential limitation in our method is when determining projected scores for athletes that have not competed in some time. Take Sunisa Lee as an example. Due to competing in college and injuries, Lee has very few scores since her Olympic performance in 2021. Because those performances account for so many of her competitions, her lower recent scores are severely overshadowed by her high, older scores. Even with weights her older scores account for a much greater proportion of her data. Thus in our methodology her projected score is unreasonably high for what recent scores suggest. This could remain true for many other athletes where their projected score does not resemble what their recent form might be. The final issue with our methodology and sampling in general is because there is technically no maximum score a gymnast could receive due to more challenging tricks being completed in every successive competition the sample could possibly get too high scores as there is no maximum cutoff in the distributions.

Some alternatives for methodology could include using a Kernal Density Estimation method to estimate the distribution, and then sampling, or obtaining all possible combinations of 5 athletes and then running an olympics simulation. We could also introduce more factors to weighting such as the age of the competitor, skill level of the competition, etc. For future work, and potentially in the final draft, we could also include a sensitivity analysis on our weights, since they were selected somewhat arbitrarily. We plan in the future to find the predicted total medal counts, which include individual all around, individual specialist, and team medals. We did not include the team medals in our predicted medal counts, but also because team medals are calculated through individual performances, optimizing for the best individual performances would necessarily optimize for the best team medal percentage. 

### The future of gymnastics: gymnast player grades, flexible optimization criterion, and rising stars

As the Olympics has become synonymous with national pride, success in Olympic gymnastics is vital to maintaining a sense of American pride. Given the high expectations laid out for the USA due to past success in the Olympics and as other countries have become more competitive on the gymnastics scene, it has become evermore important to select the team that gives the USA the best chance at being successful in Paris 2024. 

The statistical analysis done in this paper gives one-of-many statistical ways to determine that “best” team to optimize for total medal count. Our nuanced two-tier approach provides the USA Olympic committee with tangible percentages and reasonable estimates for a candidate’s estimated medal contribution, and a percentage chance to medal in a specific apparatus. Using our methodology, others might be able to further optimize for things such as team medal or overall total scores given the chance to run more simulations or adding additional data. Another use case would be replicating this for many countries, and compiling an elite database with “player grades” of sorts attached to Olympic performance. As more advanced data is measured and the intersection between sports and data analytics continue to grow, our methods and data can create a competitive advantage over the rest of the field. 

The potential for future research is endless. Our next step would be looking into optimizing lineups based on different criteria. That is, if a person wants to put a higher weighting on having the most consistent gymnasts or on the gymnast with highest potential than the model would be able to put out a lineup based on that criteria. Building out something more interactive for a final product is in the works. We will also look into more data related to the Olympics and gymnastics, and maybe interview a couple of professional gymnasts or their coaches to get further insight into Olympics level gymnastics. Finally, in the future we hope to build off similar tools to the ones created during this data analysis to predict or scout rising stars competing on the USA Elite pathway to keep an eye on for future Olympic cycles.

## Appendix A: ETFs

```{r , echo=FALSE, warning= FALSE, message=FALSE}
spyAdj = unclass(SPY$SPY.Adjusted)
ivvAdj = unclass(IVV$IVV.Adjusted)
vooAdj = unclass(VOO$VOO.Adjusted)

jotest=ca.jo(data.frame(spyAdj,ivvAdj,vooAdj), type="trace", K=2, ecdet="none", spec="longrun")
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- c("SPY", "IVV", "VOO")
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat,jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10","a = 0.05","a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

combo = jotest@V[,1][1]*spyAdj + jotest@V[,1][2]*ivvAdj + jotest@V[,1][3]*vooAdj
plot(combo, type="l")

adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type="l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of stock S over time")

abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2,2,1), lwd = c(2, 2, 2))
```

## Appendix B: Materials 

```{r , echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(materials_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- materials_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```
## Appendix C: Energy Companies

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(energy_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- energy_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

## Appendix D: Utility Companies

```{r, echo=FALSE, warning= FALSE, message=FALSE}
# Extract adjusted close prices
stock_prices <- do.call(cbind, lapply(utilities_companies, function(symbol) Cl(get(symbol))))

# Perform Johansen cointegration test
jotest <- ca.jo(stock_prices, type = "trace", K = 2, ecdet = "none", spec = "longrun")

# Display test statistic cutoffs
eigenval <- data.frame(t(jotest@lambda))
colnames(eigenval) <- utilities_companies
kable(eigenval, caption = "Eigenvalues")
cutoffs <- data.frame(jotest@teststat, jotest@cval)
colnames(cutoffs)  <- c("Test Statistic", "a = 0.10", "a = 0.05", "a = 0.01")
kable(cutoffs, caption = "Test Statistic and Cutoff values Matrix")

# Display V matrix
kable(jotest@V, caption = "Eigenvector Matrix")

# Select the first cointegrating vector coefficients
coefficients <- jotest@V[, 1]

# Combine stocks with optimized coefficients
combo <- stock_prices %*% coefficients
plot(combo, type = "l", main = "Combined Series")

# Augmented Dickey-Fuller test
adf_test_result <- adf.test(combo)
adf_table <- data.frame(
  TestStatistic = adf_test_result$statistic,
  PValue = adf_test_result$p.value
)
kable(adf_table)

# Standardize the combined series
standardized_combo <- (combo - mean(combo)) / sd(combo)
plot(standardized_combo, type = "l", xlab = "Time", ylab = "Standardized Stock", main = "Plot of Standardized Series")

# Add buy/sell signals
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "green", lty = 2)
abline(h = 0, col = "black", lty = 1)

# Add legend
legend("topright", legend = c("Sell", "Buy", "Mean"), col = c("red", "green", "black"), lty = c(2, 2, 1), lwd = c(2, 2, 2))
```

## Appendix E: EDA

```{r}
# Define stock symbols
stock_symbols <- c("MSFT", "AAPL", "AMZN", "GOOG", "GOOGL", "META")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices IT Companies",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("JNJ", "PFE", "MRK", "ABBV", "LLY")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Health Care",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("SPY", "IVV", "VOO")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices ETFs",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("JPM", "BAC", "WFC", "C", "GS")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Financials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```


```{r}
stock_symbols <- c("AMZN", "DIS", "NKE", "HD", "MCD")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Consumer Discretionary",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("META", "GOOGL", "GOOG", "T", "VZ")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Communication Services",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("BA", "HON", "UNP", "MMM", "CAT")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Industrials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("PG", "KO", "WMT", "PEP", "COST")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Consumer Staples",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("SHW", "LIN", "DOW", "APD", "ECL")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Utilities",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("NEE", "DUK", "D", "SO", "EXC")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Materials",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("AMT", "SPG", "PLD", "EQIX", "CCI")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Real Estate",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

```{r}
# Define stock symbols
stock_symbols <- c("XOM", "CVX", "COP", "KMI", "SLB")

# Fetch stock data
getSymbols(stock_symbols, from = "2021-12-12", to = "2023-12-12")

# Extract closing prices
closing_prices <- do.call(cbind, lapply(stock_symbols, function(symbol) Cl(get(symbol))))

# Plot all stock prices on one plot
matplot(index(closing_prices), closing_prices, type = "l", col = 1:length(stock_symbols),
        lty = 1, xlab = "Date", ylab = "Stock Price", main = "Stock Prices Energy",
        legend.text = stock_symbols, col.axis = "black", col.main = "black")

# Add legend
legend("topright", legend = stock_symbols, col = 1:length(stock_symbols), lty = 1)
```

## Appendix F: Graph Example of Cointegration vs Correlation
```{r, echo=FALSE, results = FALSE, warning= FALSE, message=FALSE}
set.seed(123)  # Setting seed for reproducibility
X_returns <- rnorm(100, mean = 1, sd = 1)
Y_returns <- rnorm(100, mean = 2, sd = 1)

X_diverging <- cumsum(X_returns)
Y_diverging <- cumsum(Y_returns)

# Create a data frame
diverging_data <- data.frame(X = X_diverging, Y = Y_diverging)

ggplot(diverging_data, aes(x = seq_along(X), y = X, color = "Mean 1 Cumulative returns")) +
  geom_line() +
  geom_line(aes(y = Y, color = "Mean 2 Cumulative returns")) +
  xlim(0, 99) +
  labs(title = "Diverging Time Series", x = "Time", y = "Cumulative Returns") +
  theme_minimal()
```

\newpage
### Works Cited
Cochrane, John H. “Eugene F. Fama, Efficient Markets, and The Nobel Prize.” The University of Chicago Booth School of Business, www.chicagobooth.edu/review/eugene-fama-efficient-markets-and-the-nobel-prize#:~:text=In%201970%2C%20in%20%E2%80%9CEfficient%20Capital,available%20information%20about%20future%20values. Accessed 11 Dec. 2023. 


Duffy, P. Konnor McClain enrolling at LSU this fall with hopes of balancing NCAA and elite - Gymnastics Now. Gymnastics-Now.com. 13 Jul. 2023. https://gymnastics-now.com/konnor-mcclain-enrolling-at-lsu-this-fall-with-hopes-of-balancing-ncaa-and-elite/. Accessed 20 Nov. 2023.

Gunston, Jo. “Judging the Judges – How Statistical Analysis Evaluates Fairness And ...” Olympics.Com, 19 Oct. 2023, olympics.com/en/news/how-statistical-analysis-evaluates-fairness-accuracy-gymnastics. Accessed 06 Nov. 2023.


Most popular sports in the Summer Olympics US 2021. (n.d.). Statista. https://www.statista.com/statistics/1245746/summer-olympics-most-followed-sports-us/. Accessed 20 Nov. 2023.


Olympic Summer Games: global broadcast audience. (n.d.). Statista. https://www.statista.com/statistics/280502/total-number-of-tv-viewers-of-olympic-summer-games-worldwide/#:~:text=Between%202012%20and%202020%2C%20the,3.2%20billion%20five%20years%20earlier. Accessed 20 Nov. 2023.


Olympic champ Sunisa Lee gained 45 pounds due to kidney issue. “It was so scary.” 17 Nov. 2023. USA TODAY. https://www.usatoday.com/story/sports/olympics/2023/11/17/sunisa-lee-olympic-champion-kidney-health-paris/71616483007/. Accessed 20 Nov. 2023.


Tracy, Jeff. “Poll: Gymnastics Is Americans’ Most Highly Anticipated Olympic Sport.” Axios, Morning Consult, 19 July 2021, www.axios.com/2021/07/19/olympics-favorite-sport-gymnastics. Accessed 04 Dec. 2023. 